{"batchcomplete":"","query":{"normalized":[{"from":"Bias_and_variance_tradeoff","to":"Bias and variance tradeoff"}],"redirects":[{"from":"Bias and variance tradeoff","to":"Bias\u2013variance tradeoff"}],"pages":{"40678189":{"pageid":40678189,"ns":0,"title":"Bias\u2013variance tradeoff","extract":"In statistics and machine learning, the bias\u2013variance tradeoff (or dilemma) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:\nThe bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\nThe variance is error from sensitivity to small fluctuations in the training set. High variance can cause overfitting: modeling the random noise in the training data, rather than the intended outputs.\nThe bias\u2013variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.\nThis tradeoff applies to all forms of supervised learning: classification, regression (function fitting), and structured output learning. It has also been invoked to explain the effectiveness of heuristics in human learning."}}}}